from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, UnstructuredWordDocumentLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import dotenv
import streamlit as st
import time
import os

dotenv.load_dotenv()

# Streamlit UI
st.title("ğŸ‘©â€ğŸš€RAGptğŸ‘©â€ğŸš€")

# ì‚¬ì´ë“œë°”ì— ì—…ë¡œë“œ ì°½
st.sidebar.header("íŒŒì¼ ë˜ëŠ” URL ì‚¬ìš©í•˜ê¸°")
url_input = st.sidebar.text_input("URL ì£¼ì†Œ ì…ë ¥:")
uploaded_files = st.sidebar.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["pdf", "docx", "txt"], accept_multiple_files=True)
temperature = st.sidebar.slider('Temperature', min_value=0.0, max_value=1.0, value=0.0)
top_k = st.sidebar.slider('Top K', min_value=1, max_value=10, value=1)
# ë°ì´í„° ë¡œë“œ
all_data = []

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_extension = os.path.splitext(uploaded_file.name)[1]
        start_time = time.time()
        if file_extension == ".pdf":
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
            loader = PyPDFLoader(uploaded_file.name)
            data = loader.load()
        elif file_extension == ".docx":
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
            loader = UnstructuredWordDocumentLoader(uploaded_file.name)
            data = loader.load()
        elif file_extension == ".txt":
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
            loader = TextLoader(uploaded_file.name)
            data = loader.load()

        load_time = time.time() - start_time
        st.sidebar.write(f"{uploaded_file.name} íŒŒì¼ ë¡œë“œ ì‹œê°„: {load_time:.2f} ì´ˆ")
        all_data.extend(data)

elif url_input:
    start_time = time.time()
    loader = WebBaseLoader(url_input)
    data = loader.load()
    load_time = time.time() - start_time
    st.sidebar.write(f"URL ë¡œë“œ ì‹œê°„: {load_time:.2f} ì´ˆ")
    all_data.extend(data)

# ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
if all_data:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    all_splits = text_splitter.split_documents(all_data)

    vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever(search_type="similarity_score_threshold",
                                         search_kwargs={"score_threshold": 0.6, 'k': top_k})

    from langchain.agents.agent_toolkits import create_retriever_tool

    tool = create_retriever_tool(
        retriever,
        "pdf_search",
        "ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ëŠ” PDF ë¬¸ì„œ ë‚´ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    )
    tools = [tool]

    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(temperature=temperature)

    # This is needed for both the memory and the prompt
    memory_key = "history"

    from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (
        AgentTokenBufferMemory,
    )

    memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm)

    from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
    from langchain.schema.messages import SystemMessage
    from langchain.prompts import MessagesPlaceholder

    system_message = SystemMessage(
        content=(
            "You are a nice customer service agent."
            "Do your best to answer the questions. "
            "Feel free to use any tools available to look up "
            "relevant information, only if necessary"
            "If you don't know the answer, just say you don't know. Don't try to make up an answer."
            "Make sure to answer in Korean"
        )
    )

    prompt = OpenAIFunctionsAgent.create_prompt(
        system_message=system_message,
        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],
    )

    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)

    from langchain.agents import AgentExecutor

    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        memory=memory,
        verbose=True,
        return_intermediate_steps=True,
    )

# ì±„íŒ… UI
st.header("Upload or URL & ask information U need")

# Set a default model
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ask me in the document"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ì‹¤í–‰
        if all_data:
            result = agent_executor({"input": prompt})
            for chunk in result['output'].split():
                full_response += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
        else:
            full_response = "Please upload file or valid URL for search information."
            message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
