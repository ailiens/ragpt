from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, UnstructuredWordDocumentLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import dotenv
import streamlit as st
import time
import os

dotenv.load_dotenv()

# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
current_directory = os.getcwd()
st.write(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_directory}")

# Streamlit UI
st.title("ğŸ‘©â€ğŸš€RAGptğŸ‘©â€ğŸš€")

# ì‚¬ì´ë“œë°”ì— ì—…ë¡œë“œ ì°½
st.sidebar.header("íŒŒì¼ ë˜ëŠ” URL ì‚¬ìš©í•˜ê¸°")
url_input = st.sidebar.text_input("URL ì£¼ì†Œ ì…ë ¥:")
uploaded_files = st.sidebar.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["pdf", "docx", "txt"], accept_multiple_files=True)
temperature = st.sidebar.slider('Temperature', min_value=0.0, max_value=1.0, value=0.0)
top_k = st.sidebar.slider('Top K', min_value=1, max_value=10, value=3)  # ê¸°ë³¸ê°’ ì¡°ì •
score_threshold = st.sidebar.slider('Score Threshold', min_value=0.0, max_value=1.0, value=0.5)  # ê¸°ë³¸ê°’ ì¡°ì •

# ë°ì´í„° ë¡œë“œ
all_data = []


def load_file(uploaded_file):
    file_extension = os.path.splitext(uploaded_file.name)[1]
    temp_file_path = os.path.join(current_directory, uploaded_file.name)

    with open(temp_file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if file_extension == ".pdf":
        loader = PyPDFLoader(temp_file_path)
    elif file_extension == ".docx":
        loader = UnstructuredWordDocumentLoader(temp_file_path)
    elif file_extension == ".txt":
        loader = TextLoader(temp_file_path)
    return loader.load()


if uploaded_files:
    for uploaded_file in uploaded_files:
        start_time = time.time()
        data = load_file(uploaded_file)
        for doc in data:
            doc.metadata['source'] = uploaded_file.name
        load_time = time.time() - start_time
        st.sidebar.write(f"{uploaded_file.name} íŒŒì¼ ë¡œë“œ ì‹œê°„: {load_time:.2f} ì´ˆ")
        all_data.extend(data)

    # ë©”íƒ€ë°ì´í„° í™•ì¸
    st.sidebar.header("ë©”íƒ€ë°ì´í„° í™•ì¸")
    for i, doc in enumerate(all_data):
        st.sidebar.write(f"ë¬¸ì„œ {i + 1} ë©”íƒ€ë°ì´í„°:", doc.metadata)

elif url_input:
    start_time = time.time()
    loader = WebBaseLoader(url_input)
    data = loader.load()
    load_time = time.time() - start_time
    st.sidebar.write(f"URL ë¡œë“œ ì‹œê°„: {load_time:.2f} ì´ˆ")
    all_data.extend(data)

# ChromaDB ì €ì¥ ê²½ë¡œ
CHROMA_DB_PATH = "rag_db"

# ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
if all_data:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)  # ë¶„í•  ì „ëµ ê°œì„ 
    all_splits = text_splitter.split_documents(all_data)

    # Chroma ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ê²½ë¡œ ì§€ì •
    vectorstore = Chroma.from_documents(
        documents=all_splits,
        embedding=OpenAIEmbeddings(),
        persist_directory=CHROMA_DB_PATH  # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì˜ ê²½ë¡œ ì§€ì •
    )
    retriever = vectorstore.as_retriever(search_type="similarity_score_threshold",
                                         search_kwargs={"score_threshold": score_threshold, 'k': top_k})

    from langchain.agents.agent_toolkits import create_retriever_tool

    tool = create_retriever_tool(
        retriever,
        "pdf_search",
        "ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ëŠ” PDF ë¬¸ì„œ ë‚´ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    )
    tools = [tool]

    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(temperature=temperature)

    # This is needed for both the memory and the prompt
    memory_key = "history"

    from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (
        AgentTokenBufferMemory,
    )

    memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm)

    from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
    from langchain.schema.messages import SystemMessage
    from langchain.prompts import MessagesPlaceholder

    system_message = SystemMessage(
        content=(
            "You are a nice customer service agent."
            "Do your best to answer the questions. "
            "Feel free to use any tools available to look up "
            "relevant information, only if necessary"
            "If you don't know the answer, just say you don't know. Don't try to make up an answer."
            "Make sure to answer in Korean"
        )
    )

    prompt = OpenAIFunctionsAgent.create_prompt(
        system_message=system_message,
        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],
    )

    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)

    from langchain.agents import AgentExecutor

    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        memory=memory,
        verbose=True,
        return_intermediate_steps=True,
    )

# ì±„íŒ… UI
st.header("Upload or URL & ask information U need")

# Set a default model
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ask me in the document"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        # ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ì‹¤í–‰
        if all_data:
            result = agent_executor({"input": prompt})
            full_response = result['output']
            intermediate_steps = result.get('intermediate_steps', [])

            # ì¶œì²˜ ì •ë³´ ì¶”ê°€
            sources = []
            for step in intermediate_steps:
                if 'document' in step and 'metadata' in step['document']:
                    metadata = step['document']['metadata']
                    if 'source' in metadata and 'page' in metadata:
                        sources.append(f"{metadata['source']} (page {metadata['page']})")
                    elif 'source' in metadata:
                        sources.append(f"{metadata['source']}")

            source_info = "\n\nì¶œì²˜:\n" + "\n".join(sources) if sources else "\n\nì¶œì²˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            full_response += source_info

            message_placeholder.markdown(full_response)
        else:
            full_response = "Please upload file or valid URL for search information."
            message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
